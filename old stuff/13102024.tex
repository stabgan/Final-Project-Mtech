\documentclass[12pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{color}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\geometry{margin=1in}
\setstretch{1.5}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\lhead{\small Synthetic Clinical Text Generation}
\rhead{\small \thepage}
\cfoot{}

% Code Listing Settings
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    morecomment=[l][\color{magenta}]{\#},
    breaklines=true,
    showstringspaces=false,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
}

% Begin Document
\begin{document}

% Title Page
\begin{center}
    {\Large \textbf{Generation of Synthetic Clinical Texts for ICD-10-CM Code Prediction Using Large Language Models and Knowledge Graphs}}\\[1.5cm]
    {\large Author Name}\\
    {\large Date: \today}
\end{center}

\vspace{1.5cm}

\begin{abstract}
The prediction of International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10-CM) codes from clinical narratives is a pivotal task in healthcare informatics, essential for billing, epidemiology, and clinical decision support. However, the vast number of codes and the scarcity of data for rare codes present significant challenges. This document outlines a comprehensive methodology for generating synthetic clinical texts using Large Language Models (LLMs) augmented with Medical Knowledge Graphs (MKGs). By leveraging resources such as SNOMED CT, RxNorm, the Unified Medical Language System (UMLS), and various pre-built knowledge graphs, we aim to create realistic and diverse clinical narratives that cover both common and rare ICD-10-CM codes. The synthetic data generated will enhance the training dataset for machine learning models, improving their ability to predict ICD codes accurately. This document delves into the detailed steps of the methodology, including data preparation, knowledge integration, synthetic text generation, and addresses key considerations to ensure the medical plausibility and utility of the generated texts.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}

The healthcare industry relies heavily on accurate documentation of patient encounters, which involves the assignment of standardized codes to diagnoses and procedures. The ICD-10-CM coding system is one such standard, encompassing over 70,000 codes that capture a wide array of medical conditions. Predicting these codes from unstructured clinical text is a complex task due to the diversity of language used by clinicians and the imbalance in code frequencies, especially for rare conditions. Traditional supervised learning approaches require large amounts of labeled data, which are often unavailable for rare codes.

To address this challenge, we propose a methodology that involves generating synthetic clinical texts using advanced Large Language Models (LLMs) integrated with rich Medical Knowledge Graphs (MKGs). This approach aims to augment existing datasets with high-quality, diverse clinical narratives that cover a broad spectrum of ICD-10-CM codes, thereby improving the performance of predictive models, especially for rare codes. By utilizing resources like SNOMED CT, RxNorm, and UMLS, we ensure that the generated texts are medically accurate and plausible.

In this document, we present a detailed exploration of the steps involved in this methodology, including data preparation, knowledge integration, synthetic text generation, and considerations for ensuring the quality and applicability of the generated data. We also discuss how this synthetic data can be utilized to train machine learning models for ICD-10-CM code prediction.

\section{Background}

\subsection{ICD-10-CM Coding System}

The International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10-CM) is a coding system used in the United States to classify and code all diagnoses, symptoms, and procedures recorded in conjunction with hospital care. It is an essential tool for epidemiology, health management, and clinical purposes. The system allows for greater specificity and detail in capturing health conditions compared to its predecessors.

\subsection{Challenges in ICD-10-CM Code Prediction}

Predicting ICD-10-CM codes from clinical text is challenging due to several factors:

\begin{enumerate}
    \item \textbf{High Dimensionality}: With over 70,000 codes, the classification task is inherently complex.
    \item \textbf{Class Imbalance}: Common conditions are overrepresented in clinical datasets, while rare conditions lack sufficient examples.
    \item \textbf{Unstructured Text}: Clinical notes are unstructured and contain jargon, abbreviations, and idiosyncratic language.
    \item \textbf{Ambiguity and Context Dependence}: Medical terms can be ambiguous and require context for correct interpretation.
\end{enumerate}

\section{Medical Terminologies and Knowledge Bases}

To generate synthetic clinical texts that are both realistic and medically accurate, it is crucial to utilize comprehensive medical terminologies and knowledge bases. We focus on the following resources:

\subsection{SNOMED CT}

Systematized Nomenclature of Medicine -- Clinical Terms (SNOMED CT) is a systematically organized, computer-processable collection of medical terms providing codes, terms, synonyms, and definitions used in clinical documentation and reporting. SNOMED CT covers a wide range of medical concepts, including diseases, symptoms, procedures, body structures, organisms, and substances.

SNOMED CT provides a hierarchical structure that allows for the representation of complex clinical concepts through relationships such as \textit{is a}, \textit{associated with}, and \textit{causative agent}. This rich relational information makes SNOMED CT an invaluable resource for enhancing the semantic understanding of clinical narratives and for mapping clinical concepts to standardized codes. The SNOMED CT data is typically provided in a set of tab-delimited text files, which can be imported into databases or knowledge graph structures for efficient querying.

\subsection{RxNorm}

RxNorm is a standardized nomenclature for clinical drugs produced by the U.S. National Library of Medicine (NLM). It provides normalized names for clinical drugs and links these names to many of the drug vocabularies commonly used in pharmacy management and drug interaction software. RxNorm's standardization of drug names allows for consistent representation of medications across different systems.

By incorporating RxNorm into our methodology, we can accurately represent medication information within synthetic clinical texts, ensuring that drug names, dosages, and forms are standardized and recognizable by downstream systems. RxNorm data includes details about active ingredients, drug strengths, dose forms, and brand names, which are essential for generating realistic medication information in clinical narratives.

\subsection{Unified Medical Language System (UMLS)}

The Unified Medical Language System (UMLS) is a comprehensive compendium produced by the NLM that brings together many health and biomedical vocabularies and standards to enable interoperability between computer systems. The UMLS Metathesaurus contains information about biomedical and health-related concepts, their various names, and the relationships among them.

UMLS assigns Concept Unique Identifiers (CUIs) to concepts, allowing for the mapping of terms from different vocabularies to a unified concept. This feature is particularly useful for integrating data from SNOMED CT, RxNorm, and other terminologies, facilitating the creation of a cohesive knowledge base for our application. The UMLS Metathesaurus provides rich relational data, including hierarchical relationships, synonyms, and definitions, which are critical for understanding the context and semantics of medical terms.

\section{Methodology}

Our methodology is designed to generate synthetic clinical texts that are medically accurate and cover a wide range of ICD-10-CM codes, particularly focusing on rare codes. The approach involves several key steps:

\subsection{Data Preparation and Knowledge Graph Integration}

The first step is to prepare the data and integrate various knowledge graphs to form a rich knowledge base that can be used to generate realistic clinical narratives.

\subsubsection{Optimizing Data Storage and Processing}

Given the large volume of data from medical terminologies and knowledge bases, efficient data storage and processing are crucial for performance. Leveraging high-performance data formats and tools can significantly enhance data access speeds and computational efficiency.

\paragraph{Hardware Specifications}

Our computational environment includes:

\begin{itemize}
    \item \textbf{Processor}: 8-core Ryzen 7 5800H CPU.
    \item \textbf{Memory (RAM)}: 64 GB.
    \item \textbf{Storage}: 6 TB high-speed Solid State Drive (SSD).
    \item \textbf{Graphics Processing Unit (GPU)}: NVIDIA RTX 3060 with 6 GB VRAM.
\end{itemize}

These specifications allow us to load large datasets entirely into memory and perform high-speed computations, fully utilizing the available CPU cores.

\paragraph{Data Storage Formats}

To optimize data storage and access, we convert all raw data files (e.g., CSV, TSV, Rich Release Format (RRF)) into a high-performance columnar storage format. We considered two primary formats:

\begin{enumerate}
    \item \textbf{Apache Parquet}: A columnar storage format optimized for efficient data compression and encoding schemes, enabling fast data retrieval.
    \item \textbf{Apache Arrow}: An in-memory columnar data format designed for high-performance data processing.
\end{enumerate}

\textbf{Choice of Format}: We selected \textbf{Apache Parquet} for on-disk storage due to its efficient compression, support for columnar storage, and widespread compatibility with data processing tools. Parquet files are optimized for read-heavy operations, which aligns with our need for fast data access.

\paragraph{Data Conversion and Loading}

We use \textbf{Polars}, a high-performance DataFrame library, to read the raw data files and convert them into Parquet format with appropriate data types and compression settings.

\subparagraph{Specifying Appropriate Data Types}

Defining explicit data types during data import optimizes memory usage and computational efficiency.

\begin{itemize}
    \item \textbf{Integer Types}: Use \texttt{Int32} or \texttt{Int64} depending on the data range.
    \item \textbf{Floating-Point Types}: Use \texttt{Float32} or \texttt{Float64} for numerical data.
    \item \textbf{Categorical Data}: Convert repetitive string values to \texttt{Categorical} type to reduce memory footprint.
    \item \textbf{Boolean Types}: Use \texttt{Boolean} for binary columns.
    \item \textbf{Date and Time Types}: Use \texttt{Date} or \texttt{Datetime} for temporal data.
\end{itemize}

\subparagraph{Example: Converting a CSV File to Parquet}

\begin{lstlisting}[language=Python, caption=Converting CSV to Parquet using Polars, label=lst:csv_to_parquet]
import polars as pl

# Define the schema with appropriate data types
schema = {
    'concept_id': pl.Int64,
    'term': pl.Utf8,
    'definition': pl.Utf8,
    'active': pl.Boolean,
    'effective_time': pl.Date,
    # Additional fields...
}

# Read the CSV file with specified schema
df = pl.read_csv(
    'snomed_ct_concepts.csv',
    dtypes=schema,
    parse_dates=True
)

# Write the DataFrame to Parquet format
df.write_parquet(
    'snomed_ct_concepts.parquet',
    compression='snappy'  # Use 'snappy' compression for a balance between speed and file size
)
\end{lstlisting}

\subparagraph{Compression and Encoding Options}

While storage space is not a limitation, choosing an appropriate compression codec can optimize read/write performance. We use \textbf{Snappy} compression, which offers fast compression and decompression speeds with reasonable compression ratios.

\paragraph{Loading Data into Memory}

With 64 GB of RAM, we can load the entire datasets into memory for fast access and computation.

\begin{lstlisting}[language=Python, caption=Loading Parquet files into Polars DataFrames, label=lst:load_parquet]
import polars as pl

# Load Parquet files into Polars DataFrames
snomed_df = pl.read_parquet('snomed_ct_concepts.parquet')
rxnorm_df = pl.read_parquet('rxnorm.parquet')
umls_df = pl.read_parquet('umls.parquet')
# Load additional datasets as needed
\end{lstlisting}

\paragraph{Utilizing Polars for Data Processing}

Polars allows us to perform high-speed data manipulation and analysis, leveraging parallel processing across all available CPU cores.

\subparagraph{Example: Joining DataFrames}

We perform joins to integrate data from different sources, such as mapping SNOMED CT concepts to ICD-10-CM codes.

\begin{lstlisting}[language=Python, caption=Joining SNOMED CT concepts with ICD-10-CM mappings, label=lst:join_dataframes]
# Load ICD-10-CM mappings
icd_mappings_df = pl.read_parquet('snomed_to_icd_mappings.parquet')

# Perform a left join on 'concept_id'
merged_df = snomed_df.join(
    icd_mappings_df,
    left_on='concept_id',
    right_on='snomed_concept_id',
    how='left'
)
\end{lstlisting}

\paragraph{Optimizing Hardware Utilization}

\subparagraph{Parallel Processing}

Polars automatically utilizes multiple CPU cores. We ensure that Polars is configured to use all available threads:

\begin{lstlisting}[language=Python, caption=Configuring Polars to use all CPU cores, label=lst:polars_threads]
import polars as pl

# Set Polars to use all available CPU threads
pl.Config.set_global_cfg({'polars.max_threads': 8})
\end{lstlisting}

\subparagraph{Memory Management}

We optimize memory usage by:

\begin{itemize}
    \item \textbf{Data Types}: Using appropriate data types reduces memory consumption.
    \item \textbf{Lazy Evaluation}: Utilizing Polars' lazy API for optimized query execution plans.
\end{itemize}

\paragraph{Handling Graph Data}

For graph data, such as knowledge graphs, we represent the data in edge list format using Polars DataFrames. For advanced graph algorithms, we integrate with specialized libraries like \textbf{NetworkX} for in-memory computations.

\subparagraph{Example: Creating a NetworkX Graph from Polars DataFrame}

\begin{lstlisting}[language=Python, caption=Converting edge list DataFrame to NetworkX graph, label=lst:networkx_graph]
import networkx as nx

# Assume 'edges_df' is a Polars DataFrame containing 'source' and 'target' columns
edges_df = pl.read_parquet('knowledge_graph_edges.parquet')

# Convert to pandas DataFrame for NetworkX compatibility
edges_pd = edges_df.to_pandas()

# Create the NetworkX graph
G = nx.from_pandas_edgelist(
    edges_pd,
    source='source',
    target='target'
)
\end{lstlisting}

\paragraph{Storing and Using Vector Embeddings with FAISS}

To facilitate efficient retrieval of relevant information during the generation process, we store vector embeddings in memory using \textbf{FAISS}.

\subparagraph{Generating Embeddings}

We use pre-trained biomedical language models to generate embeddings for texts.

\begin{lstlisting}[language=Python, caption=Generating embeddings using SentenceTransformer, label=lst:generate_embeddings]
from sentence_transformers import SentenceTransformer

# Load a pre-trained model
model = SentenceTransformer('all-mpnet-base-v2')

# Texts could be definitions or descriptions from SNOMED CT
texts = snomed_df['definition'].to_list()

# Generate embeddings
embeddings = model.encode(texts, convert_to_numpy=True)
\end{lstlisting}

\subparagraph{Indexing with FAISS}

\begin{lstlisting}[language=Python, caption=Creating a FAISS index for embeddings, label=lst:faiss_index]
import faiss
import numpy as np

# Determine the embedding dimension
embedding_dimension = embeddings.shape[1]

# Create a FAISS index
index = faiss.IndexFlatL2(embedding_dimension)

# Add embeddings to the index
index.add(embeddings)
\end{lstlisting}

\subparagraph{Integrating FAISS with Polars}

We store the indices returned by FAISS in Polars DataFrames to link them back to the original data.

\begin{lstlisting}[language=Python, caption=Retrieving data using FAISS indices, label=lst:faiss_retrieval]
# Perform a similarity search
query_embedding = model.encode(['Type 2 diabetes mellitus'], convert_to_numpy=True)
k = 5  # Number of nearest neighbors
distances, indices = index.search(query_embedding, k)

# Retrieve corresponding records from the DataFrame
nearest_definitions = snomed_df.take(indices[0])
\end{lstlisting}

\subparagraph{Application in Retrieval-Augmented Generation (RAG)}

The retrieved information from FAISS is used to construct prompts for the LLM, enhancing the accuracy and context of the generated clinical texts.

\paragraph{Ensuring Data Integrity and Performance}

By optimizing data storage formats, specifying appropriate data types, and leveraging high-performance libraries like Polars and FAISS, we ensure that data operations are efficient, and the system fully utilizes the available hardware resources.

\subsubsection{Integration of Medical Knowledge Graphs}

We utilize existing medical knowledge graphs such as SNOMED CT, RxNorm, UMLS, PrimeKG, and Clinical Trials Knowledge Graph (CTKG). Each of these resources provides unique and complementary information:

\begin{itemize}
    \item \textbf{SNOMED CT}: Offers detailed clinical concepts and their relationships.
    \item \textbf{RxNorm}: Provides standardized drug information.
    \item \textbf{UMLS}: Integrates multiple terminologies and provides mappings between them.
    \item \textbf{PrimeKG and CTKG}: Contain curated biomedical knowledge that can enhance the context and realism of clinical narratives.
\end{itemize}

\paragraph{Accessing the Data}

The data from SNOMED CT, RxNorm, and UMLS is typically downloaded as collections of files in specific formats (e.g., Rich Release Format (RRF) for UMLS). These files include concepts, descriptions, relationships, and mappings that can be parsed and imported into databases or graph structures.

\paragraph{Data Relevance}

All files within these datasets are relevant as they contain essential information about medical concepts, their relationships, synonyms, and mappings to other coding systems like ICD-10-CM. By parsing and integrating this data, we build a comprehensive knowledge base that supports the generation of accurate and context-rich clinical texts.

\subsubsection{Mapping SNOMED CT Concepts to ICD-10-CM Codes}

We utilize the SNOMED CT to ICD-10-CM mapping TSV file, which provides mappings between SNOMED CT concepts and ICD-10-CM codes. This mapping is essential for associating clinical concepts with the appropriate ICD codes in our synthetic texts.

\paragraph{Mathematical Representation}

Let \( S = \{s_1, s_2, ..., s_n\} \) be the set of SNOMED CT concepts, and \( I = \{i_1, i_2, ..., i_m\} \) be the set of ICD-10-CM codes. The mapping function \( f: S \rightarrow 2^I \) associates each SNOMED CT concept \( s \) with a subset of ICD-10-CM codes \( I_s \subseteq I \).

\paragraph{Example}

Consider the SNOMED CT concept "Acute myocardial infarction" (Concept ID: 22298006). The mapping file associates this concept with the ICD-10-CM code "I21.3" (ST elevation (STEMI) myocardial infarction of unspecified site).

\subsubsection{Creating Vector Embeddings with FAISS for RAG}

To facilitate efficient retrieval of relevant information during the generation process, we create vector embeddings of texts from sources like Wikipedia and PubMed articles.

\paragraph{Process}

\begin{enumerate}
    \item \textbf{Data Collection}: Gather articles from Wikipedia and PubMed that cover medical topics relevant to the ICD-10-CM codes of interest.
    \item \textbf{Preprocessing}: Clean the text data by removing irrelevant content, normalizing terms using UMLS, and tokenizing the text.
    \item \textbf{Embedding Generation}: Use a pre-trained biomedical language model (e.g., BioBERT, PubMedBERT) to generate embeddings for each document or paragraph.
    \item \textbf{Indexing with FAISS}: Utilize FAISS to index the embeddings, enabling fast similarity searches during retrieval.
\end{enumerate}

\paragraph{Mathematical Representation}

Let \( D = \{d_1, d_2, ..., d_p\} \) be the set of documents collected. For each document \( d \), we generate an embedding \( \mathbf{v}_d \in \mathbb{R}^k \) using the embedding function \( E \):

\[
\mathbf{v}_d = E(d)
\]

We index the set of embeddings \( V = \{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_p\} \) using FAISS for efficient retrieval.

\subsubsection{Preparation of Knowledge Retrieval Mechanism}

Instead of creating a unified knowledge graph, we prepare mechanisms to query multiple knowledge sources as needed. This setup allows the Retrieval-Augmented Generation (RAG) process to fetch information from various databases during text generation.

\subsection{Synthetic Clinical Text Generation Using LLMs and RAG}

The core of our methodology involves generating synthetic clinical texts using Large Language Models (LLMs) in conjunction with Retrieval-Augmented Generation (RAG).

\subsubsection{Defining ICD Code Combinations}

We aim to generate clinical texts that cover specific combinations of ICD-10-CM codes, including rare ones. To ensure medical plausibility, we select code combinations based on clinical relationships.

\paragraph{Using Knowledge Graphs to Find Clinically Plausible Code Combinations}

We leverage the relationships in SNOMED CT and other knowledge graphs to identify comorbid conditions and associated diagnoses.

\paragraph{Mathematical Representation}

Let \( C = \{c_1, c_2, ..., c_n\} \) be the set of ICD-10-CM codes. We define a relation \( R \subseteq C \times C \) where \( (c_i, c_j) \in R \) if codes \( c_i \) and \( c_j \) are clinically related.

\paragraph{Example}

If \( c_i = \) "E11.9" (Type 2 diabetes mellitus without complications) and \( c_j = \) "I10" (Essential (primary) hypertension), and there exists a relation \( (c_i, c_j) \in R \) due to their frequent comorbidity, we can consider this code combination for text generation.

\subsubsection{Retrieval-Augmented Generation (RAG) Pipeline}

The RAG approach enhances the LLM's ability to generate accurate and context-rich texts by providing it with relevant information retrieved from knowledge bases.

\paragraph{Process}

\begin{enumerate}
    \item \textbf{Input}: A set of ICD-10-CM codes \( C = \{c_1, c_2, ..., c_n\} \) and their descriptions.
    \item \textbf{Retrieval}:
    \begin{enumerate}
        \item For each code \( c_i \), retrieve related clinical information from knowledge sources:
        \begin{itemize}
            \item \textbf{Symptoms}: Retrieved from SNOMED CT concepts associated with \( c_i \).
            \item \textbf{Treatments}: Retrieved from RxNorm and clinical guidelines.
            \item \textbf{Medical History}: Common comorbid conditions and risk factors.
            \item \textbf{Laboratory Results}: Typical lab findings associated with the condition.
        \end{itemize}
        \item Retrieve relevant articles or summaries from the FAISS-indexed embeddings of Wikipedia and PubMed.
    \end{enumerate}
    \item \textbf{Prompt Construction}:
    \begin{enumerate}
        \item Compile the retrieved information into a structured prompt.
        \item Ensure that the prompt provides sufficient context for the LLM to generate a coherent and medically accurate text.
    \end{enumerate}
    \item \textbf{Generation}:
    \begin{enumerate}
        \item Input the prompt into the LLM.
        \item Generate the synthetic clinical text \( T \).
    \end{enumerate}
\end{enumerate}

\paragraph{Mathematical Representation}

Let \( R(C) \) be the retrieval function that gathers information \( I \) relevant to codes \( C \):

\[
I = R(C)
\]

The generation function \( G \) then produces the synthetic clinical text \( T \) using the LLM and the retrieved information:

\[
T = G(C, I)
\]

\subsubsection{Prompt Engineering}

Careful construction of the prompt is crucial to guide the LLM in generating realistic and medically accurate texts.

\paragraph{Example Prompt}

\textit{"Generate a detailed clinical note for a 60-year-old female patient diagnosed with type 2 diabetes mellitus without complications (ICD-10-CM: E11.9) and essential hypertension (ICD-10-CM: I10). The patient presents with symptoms of fatigue and frequent urination. Include relevant medical history, physical examination findings, laboratory results, and a treatment plan."}

\paragraph{Components of the Prompt}

\begin{itemize}
    \item \textbf{Patient Demographics}: Age, gender.
    \item \textbf{Diagnoses}: ICD-10-CM codes and their descriptions.
    \item \textbf{Symptoms}: Retrieved from knowledge bases.
    \item \textbf{Medical History}: Common risk factors and comorbidities.
    \item \textbf{Laboratory Results}: Typical findings (e.g., elevated blood glucose levels).
    \item \textbf{Treatment Plan}: Standard management strategies.
\end{itemize}

\subsubsection{Utilizing Large Language Models}

We utilize advanced LLMs capable of understanding complex medical language. Models such as GPT-4 or fine-tuned versions of T5 or GPT-3 can be used.

\paragraph{Fine-tuning Process}

\begin{enumerate}
    \item \textbf{Data Preparation}: Collect a dataset of real clinical notes (e.g., MIMIC-IV) and preprocess it to remove any Protected Health Information (PHI).
    \item \textbf{Model Selection}: Choose a pre-trained language model suitable for biomedical text.
    \item \textbf{Training}: Fine-tune the model on the clinical notes dataset, adjusting hyperparameters as necessary.
    \item \textbf{Validation}: Evaluate the model's performance on a validation set to ensure it generates coherent and contextually appropriate texts.
\end{enumerate}

\paragraph{Model Adjustments}

To improve the model's performance, we may adjust:

\begin{itemize}
    \item \textbf{Learning Rate}: Fine-tune the learning rate to prevent overfitting.
    \item \textbf{Training Epochs}: Determine the optimal number of epochs for training.
    \item \textbf{Batch Size}: Adjust the batch size to balance memory usage and convergence speed.
\end{itemize}

\subsubsection{Ensuring Medical Accuracy and Plausibility}

To maintain the quality of the synthetic texts, we implement mechanisms to verify the medical accuracy of the generated content.

\paragraph{Validation Steps}

\begin{enumerate}
    \item \textbf{Cross-Referencing}: After generation, cross-reference the clinical details with the knowledge bases to check for inconsistencies.
    \item \textbf{Automated Evaluation}: Use medical Named Entity Recognition (NER) tools to extract entities and compare them with the intended content.
    \item \textbf{Human Review}: Have clinicians review a sample of the generated texts for accuracy and plausibility.
\end{enumerate}

\paragraph{Example}

If the LLM generates a treatment plan that includes a medication contraindicated for the patient's condition, we identify this discrepancy and adjust the retrieval or prompt to prevent such errors in future generations.

\subsection{Human-in-the-Loop Feedback and Model Adjustment}

Incorporating human feedback into the generation process enhances the quality of the synthetic texts and allows the model to improve over time.

\subsubsection{Collecting Feedback from Medical Professionals}

We involve clinicians who review the generated clinical texts and provide feedback on their accuracy, realism, and usefulness.

\paragraph{Process}

\begin{enumerate}
    \item \textbf{Presentation}: Provide clinicians with generated clinical texts along with the associated ICD-10-CM codes.
    \item \textbf{Feedback Collection}: Ask clinicians to annotate the texts, highlighting any inaccuracies, missing information, or implausible elements.
    \item \textbf{Feedback Documentation}: Record the feedback in a structured format, associating comments with specific sections of the text.
\end{enumerate}

\subsubsection{Adjusting the LLM Based on Feedback}

The feedback collected is used to fine-tune the LLM further, making it more accurate in subsequent generations.

\paragraph{Process}

\begin{enumerate}
    \item \textbf{Augmenting Training Data}: Create new training examples based on the feedback, including corrected versions of the texts.
    \item \textbf{Re-fine-tuning}: Fine-tune the LLM with the augmented dataset, focusing on the areas where errors were identified.
    \item \textbf{Iterative Improvement}: Repeat the process of generation, feedback, and adjustment to continuously enhance the model's performance.
\end{enumerate}

\paragraph{Mathematical Representation}

Let \( T \) be the generated text and \( F(T) \) be the feedback function provided by clinicians. The adjusted text \( T' \) incorporates the feedback:

\[
T' = T + F(T)
\]

The updated model parameters \( \theta' \) are obtained by fine-tuning the model \( M \) using the augmented dataset \( D' \):

\[
\theta' = \operatorname{FineTune}(M, D')
\]

\subsection{Examples}

\subsubsection{Synthetic Clinical Text Generation Example}

\paragraph{Input Codes}

\begin{itemize}
    \item \textbf{ICD-10-CM Code E11.9}: Type 2 diabetes mellitus without complications
    \item \textbf{ICD-10-CM Code I10}: Essential (primary) hypertension
\end{itemize}

\paragraph{Retrieved Information}

\begin{itemize}
    \item \textbf{Symptoms}: Fatigue, frequent urination, headaches
    \item \textbf{Treatments}: Metformin, lifestyle modifications, antihypertensive medications
    \item \textbf{Relevant Medical History}: Family history of diabetes and hypertension
    \item \textbf{Laboratory Results}: Elevated fasting glucose levels, high blood pressure readings
\end{itemize}

\paragraph{Generated Clinical Text}

\textit{"Patient is a 60-year-old female presenting with fatigue and frequent urination over the past few months. She reports increased thirst and occasional headaches. Physical examination reveals a blood pressure of 150/95 mmHg. Laboratory tests indicate a fasting blood glucose level of 160 mg/dL. The patient has a family history of type 2 diabetes and hypertension. Diagnoses include type 2 diabetes mellitus without complications (E11.9) and essential hypertension (I10). The treatment plan involves initiating metformin therapy, prescribing antihypertensive medication, and recommending lifestyle modifications including diet and exercise."}

\subsubsection{Feedback and Adjustment Example}

\paragraph{Clinician Feedback}

\textit{"The clinical note is accurate, but it would be beneficial to include an assessment of the patient's lipid profile, as dyslipidemia is common in patients with diabetes and hypertension. Additionally, consider adding information about the patient's BMI and weight management recommendations."}

\paragraph{Model Adjustment}

\begin{enumerate}
    \item \textbf{Augment the Prompt}: Include requests for lipid profile results and BMI.
    \item \textbf{Update Retrieval}: Fetch information related to dyslipidemia and weight management from the knowledge bases.
    \item \textbf{Fine-tune the Model}: Incorporate the feedback into the training data and re-fine-tune the LLM.
\end{enumerate}

\paragraph{Revised Generated Clinical Text}

\textit{"Patient is a 60-year-old female presenting with fatigue and frequent urination. She also reports increased thirst and occasional headaches. Physical examination reveals a blood pressure of 150/95 mmHg and a BMI of 32 kg/m\textsuperscript{2}, indicating obesity. Laboratory tests show a fasting blood glucose level of 160 mg/dL and a lipid profile revealing elevated LDL cholesterol and triglycerides. The patient has a family history of type 2 diabetes and hypertension. Diagnoses include type 2 diabetes mellitus without complications (E11.9) and essential hypertension (I10). The treatment plan includes initiating metformin, prescribing antihypertensive and lipid-lowering medications, and recommending weight management through diet and exercise."}

\section{Considerations and Challenges}

\subsection{Medical Plausibility and Realism}

Ensuring that the synthetic clinical texts are medically plausible is paramount. We address this by:

\begin{enumerate}
    \item \textbf{Using Reliable Knowledge Sources}: All retrieved information comes from reputable medical databases and knowledge graphs.
    \item \textbf{Incorporating Clinician Feedback}: Continuous input from medical professionals helps refine the model.
    \item \textbf{Implementing Validation Checks}: Automated tools verify the presence of essential clinical elements.
\end{enumerate}

\subsection{Ethical and Legal Considerations}

While generating synthetic data mitigates patient privacy concerns, we must ensure compliance with regulations like HIPAA.

\begin{itemize}
    \item \textbf{Data Privacy}: Ensure all real patient data used for fine-tuning is de-identified.
    \item \textbf{Synthetic Data Usage}: Verify that synthetic data does not inadvertently include any real patient information.
    \item \textbf{Transparency}: Maintain documentation of the data generation process.
\end{itemize}

\subsection{Model Limitations}

LLMs can sometimes produce incorrect or nonsensical outputs. We mitigate this through:

\begin{itemize}
    \item \textbf{Fine-tuning}: Continuously refine the model with high-quality data.
    \item \textbf{Prompt Engineering}: Craft prompts that guide the model effectively.
    \item \textbf{Error Correction Mechanisms}: Implement post-generation checks to identify and correct errors.
\end{itemize}

\section{Conclusion}

By leveraging LLMs and rich medical knowledge bases, we can generate synthetic clinical texts that enhance the training datasets for ICD-10-CM code prediction models. This approach addresses the challenges of class imbalance and data scarcity for rare codes. Incorporating human feedback into the model refinement process ensures that the generated texts remain medically accurate and useful for training purposes.

\section{Future Work}

Future efforts will focus on:

\begin{itemize}
    \item \textbf{Extending to Other Medical Domains}: Applying the methodology to other coding systems like CPT or SNOMED CT.
    \item \textbf{Automating Feedback Incorporation}: Developing systems that can automatically incorporate clinician feedback.
    \item \textbf{Integrating Additional Knowledge Sources}: Including more specialized databases for rare conditions.
    \item \textbf{Evaluating Impact on Model Performance}: Conducting studies to measure how synthetic data affects prediction accuracy in real-world scenarios.
\end{itemize}

\section*{References}

\begin{enumerate}
    \item Bodenreider, O. (2004). The Unified Medical Language System (UMLS): integrating biomedical terminology. \textit{Nucleic Acids Research}, 32(suppl\_1), D267-D270.
    \item SNOMED International. (2021). \textit{SNOMED CT User Guide}.
    \item U.S. National Library of Medicine. (2021). \textit{RxNorm Overview}.
    \item Johnson, A. E. W., Pollard, T. J., et al. (2016). MIMIC-III, a freely accessible critical care database. \textit{Scientific Data}, 3(1), 1-9.
    \item Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. \textit{Proceedings of NAACL-HLT}, 4171-4186.
    \item Lee, J., Yoon, W., Kim, S., et al. (2020). BioBERT: a pre-trained biomedical language representation model for biomedical text mining. \textit{Bioinformatics}, 36(4), 1234-1240.
\end{enumerate}

\end{document}
